{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e1a78d91d52544a589e34d2b2afb0b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ImageModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ImageView",
            "format": "jpeg",
            "height": "",
            "layout": "IPY_MODEL_c01ea085149c40fab75ad9da3acbe8b9",
            "width": ""
          }
        },
        "c01ea085149c40fab75ad9da3acbe8b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "868dddd6e8d746a5b0511e3cdb30baa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ImageModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ImageView",
            "format": "jpeg",
            "height": "",
            "layout": "IPY_MODEL_a4eb2e55c95447cfba9e4c4113d77568",
            "width": ""
          }
        },
        "a4eb2e55c95447cfba9e4c4113d77568": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Pfd6XNuES3H",
        "outputId": "fd9e8ad0-a2a0-45e4-c8c0-c4f1ff820186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Collecting opencv-contrib-python (from mediapipe)\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, opencv-contrib-python, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.3\n",
            "    Uninstalling protobuf-5.29.3:\n",
            "      Successfully uninstalled protobuf-5.29.3\n",
            "Successfully installed mediapipe-0.10.21 opencv-contrib-python-4.11.0.86 protobuf-4.25.6 sounddevice-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Force GPU usage if available (ensure your Colab runtime is set to GPU)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "def euclidean_distance(pt1, pt2):\n",
        "    return np.linalg.norm(np.array(pt1) - np.array(pt2))\n",
        "\n",
        "def compute_ear(eye_points):\n",
        "    \"\"\"\n",
        "    Compute Eye Aspect Ratio (EAR) for an eye.\n",
        "    EAR = (||p2 - p6|| + ||p3 - p5||) / (2 * ||p1 - p4||)\n",
        "    \"\"\"\n",
        "    p1, p2, p3, p4, p5, p6 = eye_points\n",
        "    vertical1 = euclidean_distance(p2, p6)\n",
        "    vertical2 = euclidean_distance(p3, p5)\n",
        "    horizontal = euclidean_distance(p1, p4)\n",
        "    ear = (vertical1 + vertical2) / (2.0 * horizontal)\n",
        "    return ear\n",
        "\n",
        "def get_face_bbox(landmarks, w, h):\n",
        "    xs = [lm.x for lm in landmarks]\n",
        "    ys = [lm.y for lm in landmarks]\n",
        "    return int(min(xs) * w), int(min(ys) * h), int(max(xs) * w), int(max(ys) * h)\n",
        "\n",
        "def fancy_draw_face_points(frame, landmarks, w, h):\n",
        "    \"\"\"\n",
        "    Draw face mesh points excluding eye landmarks.\n",
        "    \"\"\"\n",
        "    # These indices correspond to the eye landmarks used for EAR calculation.\n",
        "    eye_indices = {33, 159, 160, 133, 144, 145, 263, 386, 387, 362, 373, 374}\n",
        "    num_points = len(landmarks)\n",
        "    for i, lm in enumerate(landmarks):\n",
        "        # Skip drawing if the landmark is part of an eye.\n",
        "        if i in eye_indices:\n",
        "            continue\n",
        "        x = int(lm.x * w)\n",
        "        y = int(lm.y * h)\n",
        "        hue = int(180 * i / num_points)\n",
        "        color_hsv = np.uint8([[[hue, 255, 255]]])\n",
        "        color_bgr = cv2.cvtColor(color_hsv, cv2.COLOR_HSV2BGR)[0][0].tolist()\n",
        "        cv2.circle(frame, (x, y), 1, color_bgr, -1)\n",
        "\n",
        "# ---------------- Initialization ----------------\n",
        "\n",
        "video_path = \"/content/WhatsApp Video 2025-02-23 at 11.13.23_f65644f8.mp4\"  # Update as needed\n",
        "\n",
        "# Initialize MediaPipe Face Mesh.\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(\n",
        "    static_image_mode=False,\n",
        "    max_num_faces=1,\n",
        "    refine_landmarks=True,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n",
        "\n",
        "# Create an ipywidgets Image widget for fast, in-place display.\n",
        "img_widget = widgets.Image(format='jpeg')\n",
        "display(img_widget)\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Finished processing video.\")\n",
        "        break\n",
        "\n",
        "    # Resize frame for faster processing (50% of original size)\n",
        "    frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)\n",
        "    h, w, _ = frame.shape\n",
        "\n",
        "    # Convert frame to RGB for MediaPipe processing.\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(frame_rgb)\n",
        "    sleeping = False\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        # Process only the first detected face.\n",
        "        face_landmarks = results.multi_face_landmarks[0].landmark\n",
        "\n",
        "        # Draw face landmarks, but skip drawing eye landmarks.\n",
        "        fancy_draw_face_points(frame, face_landmarks, w, h)\n",
        "\n",
        "        # Compute EAR for sleeping detection.\n",
        "        left_eye_points = [\n",
        "            (int(face_landmarks[33].x * w), int(face_landmarks[33].y * h)),\n",
        "            (int(face_landmarks[159].x * w), int(face_landmarks[159].y * h)),\n",
        "            (int(face_landmarks[160].x * w), int(face_landmarks[160].y * h)),\n",
        "            (int(face_landmarks[133].x * w), int(face_landmarks[133].y * h)),\n",
        "            (int(face_landmarks[144].x * w), int(face_landmarks[144].y * h)),\n",
        "            (int(face_landmarks[145].x * w), int(face_landmarks[145].y * h))\n",
        "        ]\n",
        "        right_eye_points = [\n",
        "            (int(face_landmarks[263].x * w), int(face_landmarks[263].y * h)),\n",
        "            (int(face_landmarks[386].x * w), int(face_landmarks[386].y * h)),\n",
        "            (int(face_landmarks[387].x * w), int(face_landmarks[387].y * h)),\n",
        "            (int(face_landmarks[362].x * w), int(face_landmarks[362].y * h)),\n",
        "            (int(face_landmarks[373].x * w), int(face_landmarks[373].y * h)),\n",
        "            (int(face_landmarks[374].x * w), int(face_landmarks[374].y * h))\n",
        "        ]\n",
        "        left_ear = compute_ear(left_eye_points)\n",
        "        right_ear = compute_ear(right_eye_points)\n",
        "        avg_ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        # Use a very small threshold so that sleeping is detected only when eyes are fully closed.\n",
        "        ear_threshold = 0.1\n",
        "        if avg_ear < ear_threshold:\n",
        "            sleeping = True\n",
        "\n",
        "        # Draw sleeping status on the frame.\n",
        "        face_xmin, face_ymin, face_xmax, face_ymax = get_face_bbox(face_landmarks, w, h)\n",
        "        status_text = \"Sleeping\" if sleeping else \"Not Sleeping\"\n",
        "        status_color = (0, 0, 255) if sleeping else (0, 255, 0)\n",
        "        cv2.putText(frame, status_text, (face_xmin, face_ymin - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, status_color, 2)\n",
        "\n",
        "    ret2, buffer = cv2.imencode('.jpg', frame)\n",
        "    if ret2:\n",
        "        img_widget.value = buffer.tobytes()\n",
        "\n",
        "    # Removed time.sleep() to speed up processing.\n",
        "\n",
        "cap.release()\n",
        "print(\"Processing complete.\")\n"
      ],
      "metadata": {
        "id": "iywgM2kUJIB3",
        "outputId": "7329dbf8-1026-4562-f937-d67036a2a874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486,
          "referenced_widgets": [
            "e1a78d91d52544a589e34d2b2afb0b37",
            "c01ea085149c40fab75ad9da3acbe8b9"
          ]
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Image(value=b'', format='jpeg')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1a78d91d52544a589e34d2b2afb0b37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished processing video.\n",
            "Processing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Force GPU usage if available\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "def euclidean_distance(pt1, pt2):\n",
        "    return np.linalg.norm(np.array(pt1) - np.array(pt2))\n",
        "\n",
        "def compute_ear(eye_points):\n",
        "    \"\"\"\n",
        "    Compute Eye Aspect Ratio (EAR) for an eye.\n",
        "    EAR = (||p2 - p6|| + ||p3 - p5||) / (2 * ||p1 - p4||)\n",
        "    \"\"\"\n",
        "    p1, p2, p3, p4, p5, p6 = eye_points\n",
        "    vertical1 = euclidean_distance(p2, p6)\n",
        "    vertical2 = euclidean_distance(p3, p5)\n",
        "    horizontal = euclidean_distance(p1, p4)\n",
        "    return (vertical1 + vertical2) / (2.0 * horizontal)\n",
        "\n",
        "def get_face_bbox(landmarks, w, h):\n",
        "    xs = [lm.x for lm in landmarks]\n",
        "    ys = [lm.y for lm in landmarks]\n",
        "    return int(min(xs) * w), int(min(ys) * h), int(max(xs) * w), int(max(ys) * h)\n",
        "\n",
        "def draw_fancy_rectangle(frame, x, y, x_max, y_max, color, thickness=2, corner_radius=20):\n",
        "    \"\"\"\n",
        "    Draws a rectangle with rounded corners.\n",
        "    \"\"\"\n",
        "    overlay = frame.copy()\n",
        "    cv2.rectangle(overlay, (x, y), (x_max, y_max), color, -1)\n",
        "    cv2.addWeighted(overlay, 0.2, frame, 0.8, 0, frame)\n",
        "\n",
        "    # Draw outer border\n",
        "    cv2.rectangle(frame, (x, y), (x_max, y_max), color, thickness, cv2.LINE_AA)\n",
        "\n",
        "def draw_fancy_status(frame, text, x, y, color):\n",
        "    \"\"\"\n",
        "    Draws a stylish status box above the face.\n",
        "    \"\"\"\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    text_size = cv2.getTextSize(text, font, 1, 2)[0]\n",
        "    text_x = x + (abs(x - x + 100) - text_size[0]) // 2  # Center text\n",
        "\n",
        "    overlay = frame.copy()\n",
        "    cv2.rectangle(overlay, (text_x - 10, y - 40), (text_x + text_size[0] + 10, y - 10), color, -1)\n",
        "    cv2.addWeighted(overlay, 0.5, frame, 0.5, 0, frame)\n",
        "\n",
        "    cv2.putText(frame, text, (text_x, y - 15), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "# ---------------- Initialization ----------------\n",
        "\n",
        "video_path = \"/content/WhatsAp9.mp4\"\n",
        "\n",
        "# Initialize MediaPipe Face Mesh\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(\n",
        "    static_image_mode=False,\n",
        "    max_num_faces=1,\n",
        "    refine_landmarks=True,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n",
        "\n",
        "# Create an ipywidgets Image widget for fast, in-place display.\n",
        "img_widget = widgets.Image(format='jpeg')\n",
        "display(img_widget)\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Finished processing video.\")\n",
        "        break\n",
        "\n",
        "    # Resize frame for faster processing (50% of original size)\n",
        "    frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)\n",
        "    h, w, _ = frame.shape\n",
        "\n",
        "    # Convert frame to RGB for MediaPipe processing.\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(frame_rgb)\n",
        "    sleeping = False\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        # Process only the first detected face.\n",
        "        face_landmarks = results.multi_face_landmarks[0].landmark\n",
        "\n",
        "        # Compute EAR for sleeping detection.\n",
        "        left_eye_points = [\n",
        "            (int(face_landmarks[33].x * w), int(face_landmarks[33].y * h)),\n",
        "            (int(face_landmarks[159].x * w), int(face_landmarks[159].y * h)),\n",
        "            (int(face_landmarks[160].x * w), int(face_landmarks[160].y * h)),\n",
        "            (int(face_landmarks[133].x * w), int(face_landmarks[133].y * h)),\n",
        "            (int(face_landmarks[144].x * w), int(face_landmarks[144].y * h)),\n",
        "            (int(face_landmarks[145].x * w), int(face_landmarks[145].y * h))\n",
        "        ]\n",
        "        right_eye_points = [\n",
        "            (int(face_landmarks[263].x * w), int(face_landmarks[263].y * h)),\n",
        "            (int(face_landmarks[386].x * w), int(face_landmarks[386].y * h)),\n",
        "            (int(face_landmarks[387].x * w), int(face_landmarks[387].y * h)),\n",
        "            (int(face_landmarks[362].x * w), int(face_landmarks[362].y * h)),\n",
        "            (int(face_landmarks[373].x * w), int(face_landmarks[373].y * h)),\n",
        "            (int(face_landmarks[374].x * w), int(face_landmarks[374].y * h))\n",
        "        ]\n",
        "        left_ear = compute_ear(left_eye_points)\n",
        "        right_ear = compute_ear(right_eye_points)\n",
        "        avg_ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        # Detect sleeping state\n",
        "        ear_threshold = 0.1\n",
        "        sleeping = avg_ear < ear_threshold\n",
        "\n",
        "        # Get face bounding box\n",
        "        face_xmin, face_ymin, face_xmax, face_ymax = get_face_bbox(face_landmarks, w, h)\n",
        "\n",
        "        # Set colors\n",
        "        status_text = \"Sleeping\" if sleeping else \"Not Sleeping\"\n",
        "        status_color = (0, 0, 255) if sleeping else (0, 255, 0)\n",
        "\n",
        "        # Draw fancy bounding box\n",
        "        draw_fancy_rectangle(frame, face_xmin, face_ymin, face_xmax, face_ymax, status_color, thickness=3)\n",
        "\n",
        "        # Draw fancy status text above the face\n",
        "        draw_fancy_status(frame, status_text, face_xmin, face_ymin, status_color)\n",
        "\n",
        "    # Encode frame for ipywidgets display\n",
        "    ret2, buffer = cv2.imencode('.jpg', frame)\n",
        "    if ret2:\n",
        "        img_widget.value = buffer.tobytes()\n",
        "\n",
        "cap.release()\n",
        "print(\"Processing complete.\")\n"
      ],
      "metadata": {
        "id": "te7eTJqjSZSi",
        "outputId": "85667f9f-f98f-4497-d3b9-d200058afecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486,
          "referenced_widgets": [
            "868dddd6e8d746a5b0511e3cdb30baa4",
            "a4eb2e55c95447cfba9e4c4113d77568"
          ]
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Image(value=b'', format='jpeg')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "868dddd6e8d746a5b0511e3cdb30baa4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished processing video.\n",
            "Processing complete.\n"
          ]
        }
      ]
    }
  ]
}